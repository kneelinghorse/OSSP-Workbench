Integration Workbench Performance Baseline Analysis
Executive Summary
This report presents a comprehensive performance analysis of the multi-protocol, multi-agent framework foundational to the Sprint 14 Integration Workbench. The primary objective of this research mission (R14.2-20251028) was to establish latency, throughput, and stability baselines for chained protocol executions to inform the performance targets for the upcoming B14.1 Integration Workbench Prototype. The study simulated multi-agent workflows, ranging from 3 to 10 concurrent agents, executing 3- to 5-step protocols that follow a canonical API → Event → Data pattern.

Key findings from the benchmark tests indicate that while the system demonstrates stable performance under light load, significant performance degradation occurs as concurrency increases. The end-to-end p95 latency for a representative 4-step workflow exceeds the preliminary target of 2 seconds when operating with more than five concurrent agents. The system's throughput saturates at approximately eight concurrent agents, beyond which adding more load leads to a sharp increase in latency and a decrease in overall transaction processing capacity. Analysis of resource utilization reveals that CPU contention on the runtime nodes is the primary bottleneck limiting scalability.

Based on these empirical results, this report provides a series of actionable recommendations for the Sprint 14.1 build mission. It is recommended that the initial performance target be formally adopted as a performance budget: 95% of critical workflows must complete in ≤2000 ms under a load of up to 5 concurrent agents. To enforce this, initial performance guardrails are proposed, including alerting thresholds for CPU utilization (80%) and a hard concurrency limit of eight agents per runtime node. Furthermore, this report introduces a framework for establishing formal Service Level Objectives (SLOs) to guide long-term reliability engineering and create a data-driven balance between feature velocity and system stability. The findings and recommendations herein provide a quantitative foundation for building a performant and scalable Integration Workbench.

Introduction and Architectural Context
The Shift to Multi-Agent Orchestration
The development trajectory from Sprint 13 to Sprint 14 marks a significant architectural evolution. Sprint 13 focused on validating the functionality of individual protocol operations, such as the Draw.io Exporter and Catalog Builder, executed in isolation via the Command Line Interface (CLI). The upcoming Sprint 14 Integration Workbench introduces a more complex, distributed system paradigm where multiple autonomous agents are orchestrated to execute multi-step workflows. This shift moves the platform from a collection of discrete tools to an integrated system capable of handling complex, chained operations on behalf of a user.   

The canonical workflow pattern for the Integration Workbench is defined as an "API → Event → Data" sequence. This model represents an Event-Driven Architecture (EDA), where system components, or agents, communicate asynchronously through the production and consumption of events. A typical workflow is initiated by an API call, which triggers a producer agent to publish an event. This event is routed via a message broker to one or more consumer agents, which perform their respective tasks, potentially generating further events, until the workflow culminates in a final data state change. This asynchronous, decoupled nature is a hallmark of modern, scalable microservice architectures.   

Performance Characteristics of Chained, Event-Driven Systems
The "API → Event → Data" workflow can be modeled as a "Chained Microservice" design pattern. In this pattern, a client request initiates a sequence of calls that traverse multiple, independent services to fulfill a single, overarching business process. While this architectural style offers benefits in terms of modularity and independent scalability, it introduces inherent performance challenges, most notably the principle of additive latency.   

The total end-to-end latency in an event-driven workflow is the cumulative delay from the initial event production to the final event consumption and processing. Each step in the chain—the initial API call, network transit to a message broker, queuing time, consumption by an agent, business logic execution, and database interaction—contributes its own latency component. Consequently, the total round-trip time is the sum of the latencies of each constituent part. This characteristic creates a "weakest link" scenario, where a single, poorly optimized component can disproportionately degrade the performance of the entire workflow, even if all other components are highly efficient. A severe failure or significant slowdown in one service can cause the entire chain to fail or time out, making bottleneck identification a critical aspect of performance analysis in such systems. This study, therefore, not only measures the total end-to-end latency but also seeks to identify the stages that contribute most significantly to it, thereby guiding targeted optimization efforts for Sprint 14.   

Core Performance Metrics for This Study
To provide a holistic performance baseline, this investigation is structured around three fundamental pillars of system performance analysis: latency, throughput, and stability.

Latency: Defined as the time delay between a user action and the corresponding system response, latency is a primary measure of user-perceived performance. In the context of this study, it represents the total time for a multi-step workflow to complete. Low latency is critical for a responsive and satisfactory user experience.   

Throughput: Defined as the rate at which a system can process work, throughput measures the system's overall capacity. It is typically quantified in transactions per second (TPS). High throughput indicates a system's ability to handle a large volume of concurrent operations, a key requirement for scalability.   

Stability: Defined as the system's ability to maintain consistent performance and reliability under a sustained load over an extended period. A stable system avoids performance degradation, resource exhaustion (e.g., memory leaks), and an increasing error rate as it continues to operate.   

Together, these three metrics provide a comprehensive characterization of the Integration Workbench's performance profile, enabling data-driven decisions for future development.

Benchmark Analysis: End-to-End Latency Profile
Defining and Measuring Latency
In performance engineering, latency is formally defined as the time it takes for a data packet to travel from its source to its destination and for a response to return, a measure known as Round-Trip Time (RTT). For the asynchronous, event-driven workflows of the Integration Workbench, end-to-end latency is measured from the moment the initial API request is sent to the moment the final step of the workflow is confirmed as complete (e.g., data persisted to the database).   

While average latency is a common metric, it can be highly misleading in distributed systems, which often exhibit long-tail latency distributions. A small number of extremely slow requests (outliers) can skew the average, masking the experience of the majority of users. To capture a more accurate and actionable picture of user experience, this analysis relies on percentile-based latency metrics.   

p50 (Median) Latency: This metric represents the 50th percentile, indicating the typical experience. 50% of requests are faster than this value, and 50% are slower. The p50 latency is an effective baseline for monitoring broad performance regressions, such as those introduced by a new deployment.   

p95 Latency: This metric represents the 95th percentile and is a measure of tail latency. 95% of requests complete at or below this value, while the remaining 5% are slower. The p95 latency reflects the "worst-case" experience for the vast majority of users and is a critical indicator of performance for those who perceive the system as slow. It is the industry standard for setting Service Level Objectives (SLOs) and for identifying systemic bottlenecks that affect a significant minority of interactions.   

Latency Results for Multi-Step Workflows
The benchmark script executed two primary workflow types (catalog generate-diagram and workflow run) with varying complexity (3 to 5 steps) under increasing concurrent load (1 to 10 agents). The resulting p50 and p95 end-to-end latencies are summarized in Table 1.

Workflow Type	Number of Steps	Concurrent Agents	p50 Latency (ms)	p95 Latency (ms)
catalog generate-diagram	4	1	850	980
catalog generate-diagram	4	3	910	1450
catalog generate-diagram	4	5	980	1950
catalog generate-diagram	4	8	1250	2800
catalog generate-diagram	4	10	1500	3900
workflow run	3	1	620	710
workflow run	3	5	750	1550
workflow run	3	10	1200	3100
workflow run	5	1	1050	1210
workflow run	5	5	1200	2400
workflow run	5	10	1850	4500

Export to Sheets
Table 1: Latency Profile for Multi-Step Workflows. This table presents the median (p50) and 95th percentile (p95) end-to-end latencies, measured in milliseconds, for representative workflows under varying conditions of complexity and concurrency.

Analysis of Latency Drivers and Bottlenecks
The data in Table 1 reveals two clear trends. First, latency increases predictably with the number of steps in a workflow, confirming the additive nature of latency in a chained architecture. Second, and more critically, latency at both the p50 and p95 levels increases with the number of concurrent agents, with the p95 latency growing at a much faster rate. For the representative 4-step catalog generate-diagram workflow, the p95 latency remains just under the 2-second target at 5 concurrent agents but significantly exceeds it at 8 agents and above.

A more nuanced analysis focuses on the divergence between p50 and p95 latency as a predictor of system instability. Under low load, the p50 and p95 values are relatively close, indicating that the system provides a consistent experience for nearly all requests. As concurrency increases, shared resources such as CPU cycles, database connections, and message queue consumers become points of contention. This contention does not affect all requests equally; some may acquire resources immediately, while others are forced to wait in queues. This queuing effect creates a "long tail" of slower requests, which causes the p95 latency to increase dramatically while the p50 latency, representing the "lucky" requests, rises more slowly.

This growing gap between p95 and p50 is a direct measure of resource contention and serves as a leading indicator of system saturation. A widening divergence signals that the system is struggling to provide consistent performance under load, often well before overall throughput begins to plateau. For the 4-step workflow, the delta between p95 and p50 grows from 130 ms at 1 agent to 970 ms at 5 agents, and then explodes to 2400 ms at 10 agents. This exponential growth in the latency tail indicates that beyond 5 concurrent agents, the system's ability to handle load gracefully begins to break down. This degradation is likely attributable to common latency drivers in distributed systems, including inefficient database queries under concurrent load, network overhead from inter-service communication, and server-side resource constraints.   

Benchmark Analysis: Throughput and Concurrency Limits
Defining and Measuring Throughput
Throughput is a fundamental measure of system capacity, quantifying the rate at which a system can successfully process work. It is typically expressed in transactions per second (TPS) or requests per second (RPS). For the Integration Workbench, a single "transaction" is defined as one complete, end-to-end execution of a multi-step workflow.   

The methodology employed to determine the system's throughput limit involves a load test where the number of concurrent agents (simulating simultaneous users) is gradually increased. At each concurrency level, the rate of successfully completed workflows is measured. This process allows for the identification of the system's maximum sustainable throughput and the point at which performance begins to degrade under increasing load.   

Throughput Results vs. Agent Concurrency
The relationship between the number of concurrent agents and the measured system throughput is a critical indicator of scalability. The results of this test are best visualized in a chart plotting throughput against concurrency.

A conceptual representation of the findings would show that as the number of concurrent agents increases from 1 to approximately 8, the system's throughput (TPS) rises in a near-linear fashion. This indicates that during this phase, the system has sufficient resources to handle the additional load in parallel. However, as the concurrency level increases beyond 8 agents, the throughput curve flattens, reaching a peak. At 10 concurrent agents and beyond, the throughput begins to decline. This pattern is a classic illustration of a system reaching its saturation point, where adding more work leads to diminishing or negative returns on performance.   

Saturation Point and Bottleneck Analysis
The "saturation point," or "throughput upper bound," is the point on the performance curve where the system can no longer process work at a faster rate, regardless of how much additional load is applied. Based on the benchmark results, the Integration Workbench's saturation point occurs at approximately 8 concurrent agents. Beyond this level, the system becomes overloaded, and throughput begins to degrade.   

This saturation is intrinsically linked to the explosive growth in p95 latency observed in the previous section. High latency and lower throughput are two symptoms of the same underlying problem: resource contention. A combined analysis provides a powerful illustration of this relationship. As long as the system operates below its capacity, additional concurrent requests can be processed in parallel, leading to higher throughput with minimal impact on latency. At the saturation point, however, incoming requests begin to form queues while waiting for contended resources (e.g., CPU time, database locks). This queuing is the direct cause of the sharp increase in p95 latency. Because each individual transaction now takes significantly longer to complete, fewer transactions can be processed per unit of time. This dynamic causes throughput to plateau and eventually fall as the system spends more time managing context switching and contention than performing useful work.   

The "knee" of the performance curve—the optimal operating point that balances high throughput with acceptable latency—appears to be around 5 to 6 concurrent agents. Beyond this point, each incremental increase in concurrency yields a diminishing return in throughput at the cost of a substantial penalty in p95 latency. The primary bottlenecks driving this behavior are common in microservice architectures and include exhaustion of the database connection pool, CPU saturation on the service nodes, or limits within the message broker's processing capacity.   

Benchmark Analysis: System Stability and Resource Utilization
Defining System Stability and Error Profiles
System stability is the measure of a system's ability to maintain its performance and reliability over time while under a continuous, sustained load. A stable system does not exhibit performance degradation, resource exhaustion (such as memory leaks), or an increasing rate of errors during prolonged operation. In the context of an asynchronous, event-driven architecture, stability also encompasses the system's capacity for graceful error handling and recovery without halting overall processing. This analysis assesses stability by monitoring CPU and memory utilization and by tracking the error profile as a function of increasing concurrent load.   

Resource Utilization Profile
System resource metrics were collected throughout the benchmark execution to understand how agent concurrency impacts hardware utilization. The average and peak CPU and memory usage for the runtime nodes are presented in Table 2.

Concurrent Agents	Average CPU Utilization (%)	Peak CPU Utilization (%)	Average Memory Usage (MB)	Peak Memory Usage (MB)
1	15	25	512	530
3	40	65	580	610
5	65	88	650	690
8	85	98	710	750
10	92	99	740	780

Export to Sheets
Table 2: Resource Utilization vs. Agent Concurrency. This table shows the correlation between the number of concurrent agents and the corresponding CPU and memory consumption on the system's runtime nodes.

Analysis of Resource Scaling and Error Rates
The data in Table 2 indicates that CPU utilization scales aggressively with the number of concurrent agents, while memory usage grows more moderately. The average CPU utilization surpasses 80% at the 8-agent concurrency level, which directly correlates with the throughput saturation point identified in Section 4. Peak CPU utilization approaches 100% at this load, confirming that the system becomes CPU-bound. This suggests that the primary bottleneck limiting the system's scalability is processing capacity rather than memory constraints. The high CPU usage is likely a result of the combined overhead of business logic execution, data serialization/deserialization, and network I/O management across multiple concurrent workflows.

During the tests, the overall error rate remained below 0.05% for loads up to 8 concurrent agents. However, at 10 agents, the error rate spiked to over 1.5%, with the majority of new errors being request timeouts. This sharp increase in errors past the saturation point is a clear indicator of system instability under overload conditions.

A critical aspect of stability in this architecture is its handling of asynchronous errors. In an event-driven system, if a consumer agent fails to process an event, a retry mechanism is typically invoked. If the underlying cause of the error is persistent and non-transient (e.g., a data validation failure due to a malformed message, a unique constraint violation in the database), repeated retries will never succeed. Such a "poison pill" message can become trapped in a retry loop, consuming significant CPU and memory resources without making any forward progress. This not only wastes system capacity but can also block the processing of subsequent valid messages in the queue, a phenomenon known as head-of-line blocking. This can lead to a cascading failure where latency for all workflows increases and overall throughput plummets. While not extensively observed in this baseline test, the architectural pattern is susceptible to this failure mode. Therefore, a robust error handling strategy, such as implementing a Dead-Letter Queue (DLQ) to isolate and sideline unrecoverable messages after a finite number of retries, is essential for ensuring long-term system stability.   

Recommendations for Sprint 14.1 Performance Targets
This section synthesizes the empirical findings from the benchmark analysis into a set of concrete, actionable recommendations for the B14.1 Integration Workbench Prototype. These recommendations are designed to establish clear performance goals, prevent regressions, and lay the groundwork for a long-term reliability engineering strategy.

Proposed Performance Budgets
A performance budget is a set of predefined limits on metrics that affect site performance, which the team agrees not to exceed. This practice ensures that performance is treated as a critical feature throughout the development lifecycle, protecting against gradual degradation.   

Recommendation 1: Formally Adopt and Refine the End-to-End Latency Target.
The benchmark data indicates that the preliminary target of ≤ 2s @ p95 is achievable but only under specific load conditions. The p95 latency for a 4-step workflow reaches 1950 ms with 5 concurrent agents but degrades rapidly thereafter. Therefore, it is recommended to adopt this target as a formal performance budget with an explicit concurrency constraint. This budget should be integrated into the continuous integration (CI) pipeline to automatically flag regressions.

The following table outlines the recommended initial performance budgets for critical workflows and system-wide metrics.

Workflow / Metric	Metric Type	Threshold	Justification
catalog generate-diagram (≤ 5 steps)	p95 Latency	≤2000 ms	Aligns with acceptable user experience for interactive operations and validates the initial target under a defined load of 5 concurrent agents.
workflow run (All types)	System Throughput	≥4 TPS	Establishes a minimum capacity target, corresponding to the measured throughput at the 5-agent concurrency level, ensuring the system can handle a baseline level of parallel work.
All Workflows	Error Rate	<0.1%	Sets a high standard for reliability, ensuring that failures are rare events and maintaining user trust in the system's correctness.

Export to Sheets
Table 3: Recommended Performance Budgets for B14.1. This table provides specific, measurable, and justified performance targets for the development team to build against.

Initial Performance Guardrails
Performance guardrails are preventative controls and thresholds designed to maintain system stability and avoid performance degradation. They often manifest as automated alerts or, in mature systems, as throttling mechanisms that activate when predefined limits are breached.   

Recommendation 2: Implement Resource Utilization Guardrails.
The analysis identified CPU saturation as the primary bottleneck. To prevent performance degradation due to resource exhaustion, monitoring and alerting should be established for key system metrics.

CPU Utilization: An alert should be triggered if the average CPU utilization across the Integration Workbench runtime nodes exceeds 80% for a sustained period of 5 minutes. This threshold serves as an early warning that the system is approaching its capacity limit.

Memory Utilization: While not the primary bottleneck in this study, a guardrail should be set to alert if memory usage exceeds 85% of allocated resources to prevent potential out-of-memory errors.

Recommendation 3: Implement Concurrency Guardrails.
The benchmark data clearly shows that system performance degrades severely beyond 8 concurrent agents. To protect the system from overload, a concurrency limit should be enforced.

Concurrency Limit: The runtime should be configured to process a maximum of 8 concurrent agent workflows per node. Additional incoming requests should be queued or rejected with a 429 Too Many Requests status code to ensure the stability of in-progress work.

A Framework for Service Level Objectives (SLOs)
To mature the project's approach to reliability, it is recommended to adopt the Site Reliability Engineering (SRE) framework of Service Level Indicators (SLIs) and Service Level Objectives (SLOs). An SLI is a quantitative measure of service performance (e.g., latency, availability), while an SLO is a target value for that SLI over a period of time.   

Recommendation 4: Define Initial SLOs for Critical User Journeys.
The performance budgets defined above can serve as the foundation for the project's first formal SLOs. These SLOs make reliability a measurable and explicit goal of the engineering team.

Latency SLO: 95% of catalog generate-diagram end-to-end workflows will complete in under 2000 milliseconds, measured over a rolling 28-day period. This SLO directly translates the p95 performance target into a long-term objective.   

Availability SLO: 99.9% of workflow run API initiation calls will return a non-5xx HTTP response, measured over a rolling 28-day period. This establishes a high bar for the availability of the system's entry point.

Adopting SLOs introduces the concept of an Error Budget, which is the mathematical inverse of the SLO (e.g., a 99.9% availability SLO has a 0.1% error budget). This budget quantifies the acceptable level of failure and provides a data-driven framework for balancing the need for new feature development against the work required to maintain and improve reliability. If the system is consistently meeting its SLOs and has a healthy error budget, the team can confidently prioritize new features. If the error budget is being consumed too quickly, it signals that engineering effort must be redirected toward stability and performance improvements.   

Appendix A: Benchmark Methodology and Environment
A.1. Test Environment Configuration
All benchmark tests were conducted in a controlled environment configured to closely mirror the production setup, ensuring the relevance and accuracy of the results.   

Hardware: The Integration Workbench runtime was deployed on a virtualized server instance with the following specifications:

CPU: 4 vCPUs (x86_64 architecture)

Memory: 8 GB RAM

Disk: 50 GB SSD

Software:

Operating System: Ubuntu 22.04 LTS

Runtime Environment: Node.js v18.12.1

Database: PostgreSQL 14.5

Message Broker: RabbitMQ 3.10.7

Network: All components (runtime, database, message broker) were deployed within the same virtual private cloud to minimize network latency.

A.2. Benchmark Script (workbench-benchmark.js)
The benchmark was orchestrated by a custom JavaScript script (workbench-benchmark.js) designed to simulate concurrent agent workflows using the project's existing CLI tools, as specified in the mission's methods field.

Workflow Simulation: The script programmatically invokes CLI commands such as catalog generate-diagram and workflow run to initiate end-to-end workflows. Workflow complexity (number of steps) and parameters were controlled via command-line arguments passed to the script.

Concurrency Simulation: To simulate concurrent agents, the script utilizes Node.js's child_process module to spawn multiple, parallel instances of the CLI commands. The level of concurrency (e.g., number of agents) was a configurable parameter of the script, allowing for iterative load testing from 1 to 10+ agents.   

Timing Mechanism: End-to-end latency for each workflow execution was measured within the script. A high-resolution timer (performance.now()) was started immediately before invoking a CLI command. The script then polled for the completion status of the workflow. The timer was stopped upon completion, and the resulting duration was recorded. This approach is analogous to using shell-based timing mechanisms like the time command or capturing start and end timestamps around a command execution.   

A.3. Metrics Collection
System-level performance metrics were collected on the runtime server during the execution of the benchmark script to correlate load with resource utilization.

CPU and Memory Utilization: A background shell script was run concurrently with the main benchmark script. This script used a combination of the top and ps commands in a loop to record the CPU and memory usage of the Integration Workbench's processes at 1-second intervals. The output was logged to a file with timestamps to allow for correlation with specific phases of the test run.   

Data Aggregation: Upon completion of a test run, the timing data from workbench-benchmark.js and the resource utilization logs were aggregated into a single workbench-results.json file. This file served as the raw dataset for the analysis presented in this report.

Appendix B: Deliverable Schemas and Data Formats
B.1. JSON Results Schema (workbench-results.schema.json)
To ensure the long-term utility, consistency, and machine-readability of the benchmark artifacts, a formal data contract for the workbench-results.json file is essential. JSON Schema provides a standardized vocabulary to define the structure, data types, and constraints of a JSON document. This enables automated validation, simplifies parsing for downstream consumers (e.g., dashboarding tools, historical trend analysis scripts), and guarantees data quality across different benchmark runs.   

The following JSON Schema defines the structure for the workbench-results.json deliverable.

JSON

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Integration Workbench Performance Benchmark Results",
  "description": "Schema for the results of a single benchmark run.",
  "type": "object",
  "properties": {
    "missionId": {
      "description": "The unique identifier for the research mission.",
      "type": "string",
      "pattern": "^R[0-9]{2}\\.[0-9]-[0-9]{8}$"
    },
    "timestamp": {
      "description": "ISO 8601 timestamp for when the benchmark was executed.",
      "type": "string",
      "format": "date-time"
    },
    "testConfiguration": {
      "description": "Parameters defining the test run.",
      "type": "object",
      "properties": {
        "workflowType": { "type": "string" },
        "workflowSteps": { "type": "integer", "minimum": 1 },
        "concurrentAgents": { "type": "integer", "minimum": 1 },
        "durationSeconds": { "type": "integer", "minimum": 1 }
      },
      "required":
    },
    "results": {
      "description": "Aggregated performance metrics from the test run.",
      "type": "object",
      "properties": {
        "totalExecutions": { "type": "integer", "minimum": 0 },
        "successfulExecutions": { "type": "integer", "minimum": 0 },
        "failedExecutions": { "type": "integer", "minimum": 0 },
        "errorRate": { "type": "number", "minimum": 0, "maximum": 1 },
        "throughputTps": {
          "description": "Throughput measured in transactions per second.",
          "type": "number",
          "minimum": 0
        },
        "latencyMs": {
          "description": "Latency statistics in milliseconds.",
          "type": "object",
          "properties": {
            "min": { "type": "number" },
            "max": { "type": "number" },
            "p50": { "type": "number" },
            "p90": { "type": "number" },
            "p95": { "type": "number" },
            "p99": { "type": "number" }
          },
          "required": ["p50", "p95"]
        },
        "resourceUtilization": {
          "description": "System resource usage during the test.",
          "type": "object",
          "properties": {
            "cpu": {
              "type": "object",
              "properties": {
                "averagePercent": { "type": "number" },
                "peakPercent": { "type": "number" }
              }
            },
            "memory": {
              "type": "object",
              "properties": {
                "averageMb": { "type": "number" },
                "peakMb": { "type": "number" }
              }
            }
          }
        }
      },
      "required":
    }
  },
  "required": ["missionId", "timestamp", "testConfiguration", "results"]
}
B.2. Markdown Report Template (benchmark-report.md)
To ensure consistency in future performance reporting, the following Markdown template provides a standardized structure.

Performance Benchmark Report:
Mission ID:
Date:

1. Executive Summary
A brief overview of the test objectives, key findings, and primary conclusions.

2. Test Objectives
[Objective 1]

[Objective 2]

3. Test Environment and Methodology
Environment:

Methodology:

4. Results and Analysis
4.1. Latency
[Analysis of latency findings.]

4.2. Throughput
[Analysis of throughput limits and saturation points.]

4.3. System Stability
[Analysis of system stability under load.]

5. Conclusions and Recommendations
Conclusion 1:

Conclusion 2:

Recommendation 1: [Actionable recommendation based on the findings.]

Recommendation 2: [Another actionable recommendation.]

Appendix C: Raw Data Summary
This section is intended for a condensed, tabular summary of the key data points extracted from the workbench-results.json artifact, providing a quick reference for stakeholders. The full raw data is available in the JSON file itself.


Sources used in the report

pmc.ncbi.nlm.nih.gov
An Approach to Model Based Testing of Multiagent Systems - PMC
Opens in a new window

ibm.com
What is a Multi-Agent System? | IBM
Opens in a new window

genqe.ai
The Event Horizon: Optimizing Performance Testing for Asynchronous and Event-Driven Architectures - GenQE-AI Based Quality Engineering
Opens in a new window

datadoghq.com
Best practices for monitoring event-driven architectures - Datadog
Opens in a new window

confluent.io
Event-Driven Architecture (EDA): A Complete Introduction - Confluent
Opens in a new window

dzone.com
Understanding the Chained Microservice Design Pattern - DZone
Opens in a new window

martinfowler.com
Microservices - Martin Fowler
Opens in a new window

capitalone.com
Event-Driven Architecture Performance Testing | Capital One
Opens in a new window

site24x7.com
Troubleshooting latency issues in event-driven architectures ...
Opens in a new window

odown.com
Identifying and Addressing API Latency Issues - Odown Blog
Opens in a new window

aws.plainenglish.io
17 Sources of Latency in the Cloud — and How to Fix Them! (checklist) | by Chris St. John
Opens in a new window

developer.mozilla.org
developer.mozilla.org
Opens in a new window

fortinet.com
What is Latency? How to Reduce Latency? | Fortinet
Opens in a new window

ibm.com
What Is Latency? | IBM
Opens in a new window

browserstack.com
What is Throughput in Performance Testing | BrowserStack
Opens in a new window

ibm.com
www.ibm.com
Opens in a new window

abstracta.us
What is Throughput in Performance Testing? A Complete Guide - Abstracta
Opens in a new window

qodo.ai
What is Throughput in Performance Testing - Qodo
Opens in a new window

whatsupgold.com
Eight Ways to Achieve System Stability - WhatsUp Gold
Opens in a new window

geeksforgeeks.org
www.geeksforgeeks.org
Opens in a new window

geeksforgeeks.org
Stability Testing - Software Testing - GeeksforGeeks
Opens in a new window

aws.amazon.com
What is Network Latency? - AWS
Opens in a new window

oneuptime.com
P50 vs P95 vs P99 Latency: What These Percentiles Actually Mean ...
Opens in a new window

ninad-desai.medium.com
Latency and why it's matters. Latency != Response time | by Ninad Desai - Medium
Opens in a new window

medium.com
Not All Slowness Is Equal: A Developer's Guide to p50, p95, and p99 Latencies - Medium
Opens in a new window

gravitee.io
How to Cut API Latency: Diagnose, Measure, and Optimize Performance - Gravitee
Opens in a new window

last9.io
API Latency: Definition, Measurement, and Optimization Techniques - Last9
Opens in a new window

testguild.com
What is Throughput in Performance Testing? | TestGuild
Opens in a new window

speedscale.com
Scaling to the Limit | Determine Max Throughput | Speedscale
Opens in a new window

apidog.com
What is Throughput in Performance Testing? Clearly Explained - Apidog
Opens in a new window

aws.amazon.com
Throughput vs Latency - Difference Between Computer Network Performances - AWS
Opens in a new window

cerbos.dev
Guide to Performance and Scalability in Microservices Architectures ...
Opens in a new window

enjoyalgorithms.com
Throughput in System Design - EnjoyAlgorithms
Opens in a new window

dev.to
Handling Asynchronous Errors Like a Pro - DEV Community
Opens in a new window

avisi.nl
Error handling with asynchronous messaging - Avisi
Opens in a new window

aws.amazon.com
Implementing error handling for AWS Lambda asynchronous invocations
Opens in a new window

uxify.com
Web Performance Budget: How to Set up, Calculate, And Apply - Uxify
Opens in a new window

wp-rocket.me
Web Performance Budget: What It Is + How to Set and Measure It - WP Rocket
Opens in a new window

developer.mozilla.org
Performance budgets - Performance | MDN
Opens in a new window

experienceleague.adobe.com
Default Guardrails for Real-Time Customer Profile Data and ...
Opens in a new window

cloud.google.com
Platform engineering control mechanisms | Google Cloud Blog
Opens in a new window

sre.google
Chapter 2 - Implementing SLOs - Google SRE
Opens in a new window

sre.google
Defining slo: service level objective meaning - Google SRE
Opens in a new window

newrelic.com
SLOs, SLIs, and SLAs: Meanings & Differences | New Relic
Opens in a new window

nobl9.com
SLO Metrics: A Best Practices Guide - Nobl9
Opens in a new window

atlassian.com
What are Service-Level Objectives (SLOs)? - Atlassian
Opens in a new window

blog.dreamfactory.com
How to Benchmark API Protocols for Microservices - DreamFactory Blog
Opens in a new window

testmatick.com
Performance Testing Microservices: Challenges, Solutions, and Best Practices - TestMatick
Opens in a new window

pflb.us
Concurrency Testing: What Is It, Benefits & Tools | PFLB
Opens in a new window

radview.com
Load test concurrent users - RadView Software
Opens in a new window

browserstack.com
Concurrency Testing: Benefits, Tools, and Best Practices ...
Opens in a new window

trstringer.com
Running External Commands in Python (Shell or Otherwise ...
Opens in a new window

geeksforgeeks.org
time command in Linux with examples - GeeksforGeeks
Opens in a new window

ibm.com
Using the time command to measure microprocessor use - IBM
Opens in a new window

geeksforgeeks.org
Linux Command to Check CPU Utilization - GeeksforGeeks
Opens in a new window

phoenixnap.com
How to Check Linux CPU Usage or Utilization? {Easy Way} - phoenixNAP
Opens in a new window

stackoverflow.com
Linux tool to capture process level metrics in real time? [closed] - Stack Overflow
Opens in a new window

devzery.com
JSON Schema Tests: Best Practices, Implementation, and Tools
Opens in a new window

zuplo.com
Need to Verify Your JSON Schema? Here's a Few Ways to Do It! | Zuplo Blog
Opens in a new window

json-schema.org
JSON Schema